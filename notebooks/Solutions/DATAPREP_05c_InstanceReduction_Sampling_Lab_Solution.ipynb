{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the wine dataset to perform a simple split validation with different training and test ratios:\n",
    "- 50% training data  / 50% test data\n",
    "- 60% training data  / 40% test data\n",
    "- 70% training data  / 30% test data\n",
    "- 80% training data  / 20% test data\n",
    "\n",
    "\n",
    "1. Randomly split iris data data into training and test sample using the train_test_split function\n",
    "2. Print the number of resulting training and test examples to check\n",
    "3. Train an SVM classifier using the training sample\n",
    "4. Apply the trained model to the test sample, calculating the accuracy (use the score method of the classifier)\n",
    "5. Iterate over the different training/test splits printing the test accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of original Wine Data Set:  (178, 13) (178,)\n",
      "\n",
      "Training examples:  89 , test examples:  89\n",
      "Accuracy of classifier on  50.0 % testing sample:  0.9550561797752809\n",
      "\n",
      "Training examples:  106 , test examples:  72\n",
      "Accuracy of classifier on  40.0 % testing sample:  0.9583333333333334\n",
      "\n",
      "Training examples:  124 , test examples:  54\n",
      "Accuracy of classifier on  30.0 % testing sample:  0.9629629629629629\n",
      "\n",
      "Training examples:  142 , test examples:  36\n",
      "Accuracy of classifier on  20.0 % testing sample:  0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "#Different splitting and validation approaches\n",
    "#Simple split validation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "\n",
    "#load wine data set\n",
    "x, y = datasets.load_wine(return_X_y=True)\n",
    "print(\"Shape of original Wine Data Set: \",x.shape, y.shape)\n",
    "\n",
    "test_splits=[0.5,0.4,0.3,0.2]\n",
    "\n",
    "for test_split in test_splits:\n",
    "    #Randomly split data into training and test sample (data is shuffled by default)\n",
    "    #Data is sampled without replacement\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_split, random_state=1)\n",
    "\n",
    "    #Print the number of resulting training and test examples to check\n",
    "    print(\"\\nTraining examples: \",x_train.shape[0],\", test examples: \",x_test.shape[0])\n",
    "\n",
    "    #Train an SVM classifier using the training sample\n",
    "    clf = svm.SVC(kernel='linear', C=1).fit(x_train, y_train)\n",
    "    #Apply the trained model to the test sample, calculating the accuracy (use the score method of the classifier)\n",
    "    print(\"Accuracy of classifier on \",test_split*100,\"% test sample: \",clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will add stratified sampling which enforces a representative sample concerning classes\n",
    "\n",
    "Encapsulate the split validation you just created into a function named \"split_validation\"\n",
    "- parameters: features (the features), label (the label), test_split (the testing ratio), stratify_flag (y or n)\n",
    "- no return value\n",
    "\n",
    "Within the split_validation function, modify the parameters of the train_test_split function using the parameter \"stratify\" to enable stratified sampling.\n",
    "\n",
    "Call the function from a for loop using different test_splits again like in the last task and let the function calculate and print test accuracies again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training examples:  89 , test examples:  89\n",
      "Accuracy of classifier on  50.0 % test sample:  0.9213483146067416\n",
      "\n",
      "Training examples:  106 , test examples:  72\n",
      "Accuracy of classifier on  40.0 % test sample:  0.9305555555555556\n",
      "\n",
      "Training examples:  124 , test examples:  54\n",
      "Accuracy of classifier on  30.0 % test sample:  0.9259259259259259\n",
      "\n",
      "Training examples:  142 , test examples:  36\n",
      "Accuracy of classifier on  20.0 % test sample:  0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "#Variation Stratified Sampling: Enforces a representative sample concerning classes\n",
    "\n",
    "def split_validation(features, label, test_split,stratify_flag):\n",
    "    #Randomly split data into training and test sample (data is shuffled by default)\n",
    "    #Data is sampled without replacement\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features, label, test_size=test_split, random_state=1, stratify=stratify_flag)\n",
    "\n",
    "    #Print the number of resulting training and test examples to check\n",
    "    print(\"\\nTraining examples: \",x_train.shape[0],\", test examples: \",x_test.shape[0])\n",
    "\n",
    "    #Train an SVM classifier using the training sample\n",
    "    clf = svm.SVC(kernel='linear', C=1).fit(x_train, y_train)\n",
    "    #Apply the trained model to the test sample, calculating the accuracy (use the score method of the classifier)\n",
    "    print(\"Accuracy of classifier on \",test_split*100,\"% test sample: \",clf.score(x_test, y_test))\n",
    "\n",
    "for test_split in test_splits:\n",
    "    split_validation(x, y, test_split,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, data is often split into three samples:\n",
    "- training sample (just for training)\n",
    "- validation sample (to check the accuracy of different models during the training phase)\n",
    "- test sample (to check the accuracy of the final model)\n",
    "\n",
    "Use the train_test_split to split the wine dataset into 75% training data, 15% validation data and 10% test data!\n",
    "\n",
    "Remark: You will not get the exact splits required - think about why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall number of examples in wine data set 178\n",
      "Calculated percentage of training examples:  74.71910112359551 %\n",
      "Calculated percentage of validation examples:  15.168539325842698 %\n",
      "Calculated percentage of test examples:  10.112359550561797 %\n"
     ]
    }
   ],
   "source": [
    "#Variation Train-Validation-Test Sampling\n",
    "train_split=0.75\n",
    "test_split=0.1\n",
    "val_split=1-train_split-test_split\n",
    "\n",
    "#Split wine data set into training, test and validation examples\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(x, y, test_size=test_split, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=val_split/(train_split+val_split), random_state=1)\n",
    "\n",
    "#Print Overall number of examples and calculated! percentages of the splits \n",
    "total=x.shape[0]\n",
    "print(\"Overall number of examples in wine data set\",total)\n",
    "print(\"Calculated percentage of training examples: \",x_train.shape[0]/total*100,\"%\")\n",
    "print(\"Calculated percentage of validation examples: \",x_val.shape[0]/total*100,\"%\")\n",
    "print(\"Calculated percentage of test examples: \",x_test.shape[0]/total*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Cross Validation, training data is split multiple times into training and validation sub sets.\n",
    "\n",
    "Cross validation resamples without replacement and is primarily used to verify the hyperparameters of the model (and not as an ensemble method)\n",
    "\n",
    "\n",
    "1. Define an SVM classifier using the svm.SVC class\n",
    "\n",
    "2. Perform cross validation with k=5 using cross_val_score\n",
    "\n",
    "3. Print the calculated accuracies / scores\n",
    "\n",
    "4. Print the mean value of the calculated accuracies and its standard deviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies:  [0.96666667 1.         0.96666667 0.96666667 1.        ]\n",
      "Accuracy: 0.98 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "#K-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "\n",
    "#Define an SVM classifier using the svm.SVC class\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "\n",
    "#Perform cross validation with k=5 using cross_val_score\n",
    "scores = cross_val_score(clf, x, y, cv=5)\n",
    "#Print the calculated accuracies / scores\n",
    "print(\"Accuracies: \",scores)\n",
    "#Print the mean value of the calculated accuracies and its standard deviation\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to combine models that are trained on different subsets of the training examples, we can use a variation of the Bagging (Bootstrap AGGregatING) approach\n",
    "\n",
    "1. Load iris data set (150 examples, can be split easier to proper samples :)\n",
    "\n",
    "2. Split iris data set into training and test examples using train_test_split as before test split=0.2\n",
    "\n",
    "3. Define a new classifier using the BaggingClassifier class with k=6 folds and bootstrap=False\n",
    "\n",
    "4. Fit your bagging classifier\n",
    "\n",
    "5. Apply the trained bagging model on test data\n",
    "\n",
    "6. Combine features, label and predicted label to a new 2-dim array and print the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of original iris Data Set:  (150, 4) (150,)\n",
      "[[5.8 4.  1.2 0.2 0.  0. ]\n",
      " [5.1 2.5 3.  1.1 1.  1. ]\n",
      " [6.6 3.  4.4 1.4 1.  1. ]\n",
      " [5.4 3.9 1.3 0.4 0.  0. ]\n",
      " [7.9 3.8 6.4 2.  2.  2. ]\n",
      " [6.3 3.3 4.7 1.6 1.  1. ]\n",
      " [6.9 3.1 5.1 2.3 2.  2. ]\n",
      " [5.1 3.8 1.9 0.4 0.  0. ]\n",
      " [4.7 3.2 1.6 0.2 0.  0. ]\n",
      " [6.9 3.2 5.7 2.3 2.  2. ]\n",
      " [5.6 2.7 4.2 1.3 1.  1. ]\n",
      " [5.4 3.9 1.7 0.4 0.  0. ]\n",
      " [7.1 3.  5.9 2.1 2.  2. ]\n",
      " [6.4 3.2 4.5 1.5 1.  1. ]\n",
      " [6.  2.9 4.5 1.5 1.  1. ]\n",
      " [4.4 3.2 1.3 0.2 0.  0. ]\n",
      " [5.8 2.6 4.  1.2 1.  1. ]\n",
      " [5.6 3.  4.5 1.5 1.  1. ]\n",
      " [5.4 3.4 1.5 0.4 0.  0. ]\n",
      " [5.  3.2 1.2 0.2 0.  0. ]\n",
      " [5.5 2.6 4.4 1.2 1.  1. ]\n",
      " [5.4 3.  4.5 1.5 1.  1. ]\n",
      " [6.7 3.  5.  1.7 1.  2. ]\n",
      " [5.  3.5 1.3 0.3 0.  0. ]\n",
      " [7.2 3.2 6.  1.8 2.  2. ]\n",
      " [5.7 2.8 4.1 1.3 1.  1. ]\n",
      " [5.5 4.2 1.4 0.2 0.  0. ]\n",
      " [5.1 3.8 1.5 0.3 0.  0. ]\n",
      " [6.1 2.8 4.7 1.2 1.  1. ]\n",
      " [6.3 2.5 5.  1.9 2.  2. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#load iris data set (150 examples, can be split easier to proper samples :)\n",
    "x, y = datasets.load_iris(return_X_y=True)\n",
    "print(\"Shape of original iris Data Set: \",x.shape, y.shape)\n",
    "\n",
    "#Split iris data set into training and test examples using train_test_split as before test split=0.2\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "\n",
    "#Define a new classifier using the BaggingClassifier class with k=6 folds and bootstrap=False\n",
    "#Bagging without replacement (bootstrap=False) resembles the fitting performed during K-Fold cross validation\n",
    "clf = BaggingClassifier(base_estimator=svm.SVC(),n_estimators=6, random_state=0,bootstrap=False)\n",
    "#Fit your bagging classifier\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "#Apply the trained bagging model on test data \n",
    "y_test_pred=clf.predict(x_test)\n",
    "#Combine features, label and predicted label to a new 2-dim array and print the result\n",
    "test_comp=np.concatenate((x_test.reshape(30,4),y_test.reshape(30,1),y_test_pred.reshape(30,1)),axis=1)\n",
    "print(test_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to resampling without replacement (which we just performed), bootstrapping resamples with replacement.\n",
    "\n",
    "This means, that an example can be part of multiple bootstrap samples.\n",
    "\n",
    "Let's demonstrate this with a simple data set\n",
    "\n",
    "1. Perform bootstrapping with 10 samples using a loop\n",
    "\n",
    "2. To create a single sample, use the resample function drawing 4 examples with replacement\n",
    "\n",
    "3. Print the example within your sample\n",
    "   \n",
    "4. Print the out of bag observations - any example, which is in the original data, but not in the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
      "Bootstrapping with k=10. \n",
      "Number of instances in OOB sample depends on number of repeatedly drawn instances in bootstrap sample!\n",
      "\n",
      "k= 1 Bootstrap Sample: [0.6, 0.4, 0.6, 0.3]\n",
      "OOB Sample: [0.1, 0.2, 0.5]\n",
      "\n",
      "k= 2 Bootstrap Sample: [0.4, 0.2, 0.5, 0.3]\n",
      "OOB Sample: [0.1, 0.6]\n",
      "\n",
      "k= 3 Bootstrap Sample: [0.4, 0.3, 0.4, 0.1]\n",
      "OOB Sample: [0.2, 0.5, 0.6]\n",
      "\n",
      "k= 4 Bootstrap Sample: [0.3, 0.6, 0.4, 0.6]\n",
      "OOB Sample: [0.1, 0.2, 0.5]\n",
      "\n",
      "k= 5 Bootstrap Sample: [0.1, 0.6, 0.5, 0.1]\n",
      "OOB Sample: [0.2, 0.3, 0.4]\n",
      "\n",
      "k= 6 Bootstrap Sample: [0.1, 0.4, 0.3, 0.5]\n",
      "OOB Sample: [0.2, 0.6]\n",
      "\n",
      "k= 7 Bootstrap Sample: [0.5, 0.2, 0.4, 0.6]\n",
      "OOB Sample: [0.1, 0.3]\n",
      "\n",
      "k= 8 Bootstrap Sample: [0.5, 0.4, 0.1, 0.4]\n",
      "OOB Sample: [0.2, 0.3, 0.6]\n",
      "\n",
      "k= 9 Bootstrap Sample: [0.1, 0.6, 0.4, 0.6]\n",
      "OOB Sample: [0.2, 0.3, 0.5]\n",
      "\n",
      "k= 10 Bootstrap Sample: [0.3, 0.6, 0.6, 0.1]\n",
      "OOB Sample: [0.2, 0.4, 0.5]\n"
     ]
    }
   ],
   "source": [
    "#Bootstrapping\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# data sample\n",
    "data = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "print('Original data: ',data)\n",
    "\n",
    "print(\"Bootstrapping with k=10. \\nNumber of instances in OOB sample depends on number of repeatedly drawn instances in bootstrap sample!\")\n",
    "\n",
    "#Perform bootstrapping with 10 samples using a loop\n",
    "for i in range(1,11):\n",
    "#To create a single sample, use the resample function drawing 4 examples with replacement\n",
    "    boot = resample(data, replace=True, n_samples=4)\n",
    "#Print the example within your sample\n",
    "    print('\\nk= %i Bootstrap Sample: %s' % (i,boot))\n",
    "\n",
    "# Print the out of bag observations - any example, which is in the original data, but not in the sample\n",
    "    oob = [x for x in data if x not in boot]\n",
    "    print('OOB Sample: %s' % oob)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
